{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer 'music_highlighter' (type MusicHighlighter).\n\n{{function_node __wrapped__Squeeze_device_/job:localhost/replica:0/task:0/device:CPU:0}} Can not squeeze dim[1], expected a dimension of 1, got 0 [Op:Squeeze] name: \n\nCall arguments received by layer 'music_highlighter' (type MusicHighlighter):\n  • x=tf.Tensor(shape=(1, 100, 128), dtype=float32)\n  • pos_enc=tf.Tensor(shape=(1, 100, 256), dtype=float32)\n  • num_chunk=100",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 83\u001b[0m\n\u001b[0;32m     80\u001b[0m y_true_sample \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mone_hot(tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform([\u001b[38;5;241m1\u001b[39m], maxval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m190\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32), \u001b[38;5;241m190\u001b[39m)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Run a training step\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_enc_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_chunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_true_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "Cell \u001b[1;32mIn[2], line 67\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, x, pos_enc, num_chunk, y_true, optimizer)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(model, x, pos_enc, num_chunk, y_true, optimizer):\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 67\u001b[0m         y_pred, attn_score \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_chunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m         loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompute_loss(y_true, y_pred)\n\u001b[0;32m     69\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[1;32me:\\Highlight detect\\Jupyter\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[2], line 34\u001b[0m, in \u001b[0;36mMusicHighlighter.call\u001b[1;34m(self, x, pos_enc, num_chunk)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Global max-pooling\u001b[39;00m\n\u001b[0;32m     33\u001b[0m net \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_max(net, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Restore shape [batch_size, num_chunk, dim_feature * 4]\u001b[39;00m\n\u001b[0;32m     37\u001b[0m net \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(net, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_chunk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_feature \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m])\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'music_highlighter' (type MusicHighlighter).\n\n{{function_node __wrapped__Squeeze_device_/job:localhost/replica:0/task:0/device:CPU:0}} Can not squeeze dim[1], expected a dimension of 1, got 0 [Op:Squeeze] name: \n\nCall arguments received by layer 'music_highlighter' (type MusicHighlighter):\n  • x=tf.Tensor(shape=(1, 100, 128), dtype=float32)\n  • pos_enc=tf.Tensor(shape=(1, 100, 256), dtype=float32)\n  • num_chunk=100"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MusicHighlighter(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MusicHighlighter, self).__init__()\n",
    "        self.dim_feature = 64\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(self.dim_feature, (3, 129), strides=(2, 1), activation='relu')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(self.dim_feature * 2, (4, 1), strides=(2, 1), activation='relu')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(self.dim_feature * 4, (4, 1), strides=(2, 1), activation='relu')\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = tf.keras.layers.Dense(self.dim_feature * 4, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(self.dim_feature * 4, activation='relu')\n",
    "        self.fc3 = tf.keras.layers.Dense(1024, activation='relu')\n",
    "        self.fc4 = tf.keras.layers.Dense(190, activation='softmax')\n",
    "        \n",
    "        # Attention mechanism layers\n",
    "        self.attn_fc1 = tf.keras.layers.Dense(self.dim_feature * 4, activation='tanh')\n",
    "        self.attn_fc2 = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, x, pos_enc, num_chunk):\n",
    "        # Expand dimensions to create [batch, time, frequency, channels]\n",
    "        net = tf.expand_dims(x, axis=-1)\n",
    "        \n",
    "        # 2D Conv. feature extraction\n",
    "        net = self.conv1(net)\n",
    "        net = self.conv2(net)\n",
    "        net = self.conv3(net)\n",
    "        \n",
    "        # Global max-pooling\n",
    "        net = tf.reduce_max(net, axis=1)\n",
    "        net = tf.squeeze(net, axis=1)\n",
    "        \n",
    "        # Restore shape [batch_size, num_chunk, dim_feature * 4]\n",
    "        net = tf.reshape(net, [-1, num_chunk, self.dim_feature * 4])\n",
    "\n",
    "        # Apply positional encoding\n",
    "        attn_net = net + pos_enc\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attn_net = self.fc1(attn_net)\n",
    "        attn_net = self.fc2(attn_net)\n",
    "        attn_score = self.attention(attn_net, self.dim_feature * 4)\n",
    "\n",
    "        # Compute final predictions\n",
    "        net = self.fc3(net)\n",
    "        chunk_predictions = self.fc4(net)\n",
    "        \n",
    "        # Compute weighted sum with attention scores\n",
    "        overall_predictions = tf.squeeze(tf.matmul(attn_score, chunk_predictions, transpose_a=True), axis=1)\n",
    "        return overall_predictions, attn_score\n",
    "\n",
    "    def attention(self, inputs, dim):\n",
    "        outputs = self.attn_fc1(inputs)\n",
    "        outputs = self.attn_fc2(outputs)\n",
    "        attn_score = tf.nn.softmax(outputs, axis=1)\n",
    "        return attn_score\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred + 1e-10), axis=1))\n",
    "\n",
    "# Example usage\n",
    "def train_step(model, x, pos_enc, num_chunk, y_true, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred, attn_score = model(x, pos_enc, num_chunk)\n",
    "        loss = model.compute_loss(y_true, y_pred)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Sau khi khởi tạo mô hình\u001b[39;00m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m MusicHighlighter()\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Highlight detect\\Jupyter\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py:3403\u001b[0m, in \u001b[0;36mModel.summary\u001b[1;34m(self, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[0;32m   3372\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prints a string summary of the network.\u001b[39;00m\n\u001b[0;32m   3373\u001b[0m \n\u001b[0;32m   3374\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3400\u001b[0m \u001b[38;5;124;03m    ValueError: if `summary()` is called before the model is built.\u001b[39;00m\n\u001b[0;32m   3401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[1;32m-> 3403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3404\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has not yet been built. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3405\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe model on a batch of data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3407\u001b[0m     )\n\u001b[0;32m   3408\u001b[0m layer_utils\u001b[38;5;241m.\u001b[39mprint_summary(\n\u001b[0;32m   3409\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3410\u001b[0m     line_length\u001b[38;5;241m=\u001b[39mline_length,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3415\u001b[0m     layer_range\u001b[38;5;241m=\u001b[39mlayer_range,\n\u001b[0;32m   3416\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
     ]
    }
   ],
   "source": [
    "# Sau khi khởi tạo mô hình\n",
    "\n",
    "model = MusicHighlighter()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicHighlighter(object):\n",
    "    def __init__(self):\n",
    "        self.dim_feature = 64\n",
    "\n",
    "        # During training or testing, we both use batch normalization\n",
    "        self.is_training = tf.compat.v1.placeholder_with_default(False, shape=(), name='is_training')\n",
    "        self.bn_params = {'is_training': self.is_training,\n",
    "                          'center': True, 'scale': True,\n",
    "                          'updates_collections': None}\n",
    "\n",
    "        # Placeholders for input data\n",
    "        self.x = tf.compat.v1.placeholder(tf.float32, shape=[None, None, 128], name='x')\n",
    "        self.pos_enc =  tf.compat.v1.placeholder(tf.float32, shape=[None, None, self.dim_feature * 4], name='pos_enc')\n",
    "        self.num_chunk =  tf.compat.v1.placeholder(tf.int32, name='num_chunk')\n",
    "\n",
    "        # Build the model\n",
    "        self.build_model()\n",
    "    def conv(self, inputs, filters, kernel, stride):\n",
    "        dim = inputs.get_shape().as_list()[-2]\n",
    "        return slim.conv2d(inputs, filters,\n",
    "                           [kernel, dim], [stride, dim],\n",
    "                           activation_fn=tf.nn.relu,\n",
    "                           normalizer_fn=slim.batch_norm,\n",
    "                           normalizer_params=self.bn_params)\n",
    "\n",
    "    def fc(self, inputs, num_units, act=tf.nn.relu):\n",
    "        return slim.fully_connected(inputs, num_units,\n",
    "                                    activation_fn=act,\n",
    "                                    normalizer_fn=slim.batch_norm,\n",
    "                                    normalizer_params=self.bn_params)\n",
    "\n",
    "    def attention(self, inputs, dim):\n",
    "        outputs = self.fc(inputs, dim, act=tf.nn.tanh)\n",
    "        outputs = self.fc(outputs, 1, act=None)\n",
    "        attn_score = tf.nn.softmax(outputs, axis=1)\n",
    "        return attn_score\n",
    "\n",
    "    def build_model(self):\n",
    "        # 2D Conv. feature extraction\n",
    "        net = tf.expand_dims(self.x, axis=3)  # [batch_size, time, 128, 1]\n",
    "        net = self.conv(net, self.dim_feature, 3, 2)\n",
    "        net = self.conv(net, self.dim_feature * 2, 4, 2)\n",
    "        net = self.conv(net, self.dim_feature * 4, 4, 2)\n",
    "\n",
    "        # Global max-pooling\n",
    "        net = tf.squeeze(tf.reduce_max(net, axis=1), axis=1)\n",
    "\n",
    "        # Restore shape [batch_size, num_chunk, dim_feature*4]\n",
    "        net = tf.reshape(net, [1, self.num_chunk, self.dim_feature * 4])\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_net = net + self.pos_enc\n",
    "        attn_net = self.fc(attn_net, self.dim_feature * 4)\n",
    "        attn_net = self.fc(attn_net, self.dim_feature * 4)\n",
    "        self.attn_score = self.attention(attn_net, self.dim_feature * 4)\n",
    "        # This part is only used in training ##\n",
    "        # net = self.fc(net, 1024)\n",
    "        # chunk_predictions = self.fc(net, 18, act=tf.nn.sigmoid)\n",
    "        # overall_predictions = tf.squeeze(tf.matmul(self.attn_score, chunk_predictions, transpose_a=True), axis=1)\n",
    "        # loss = tf.reduce_mean(-tf.reduce_sum(self.y * tf.log(overall_predictions), axis=1))\n",
    "        # Initialize the Saver to restore the model later\n",
    "        self.saver = tf.compat.v1.train.Saver(tf.compat.v1.global_variables())\n",
    "\n",
    "    def calculate(self, sess, x, pos_enc, num_chunk):\n",
    "        # Feed input data and calculate attention score\n",
    "        feed_dict = {self.x: x, self.pos_enc: pos_enc, self.num_chunk: num_chunk, self.is_training: False}\n",
    "        attn_score = sess.run(self.attn_score, feed_dict=feed_dict)\n",
    "        return attn_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
